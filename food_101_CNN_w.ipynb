{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyMqZ31LKxF2w6holKF1/X3N"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jc7aN1JBvvdX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score,accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIzQkoB5fmuL",
        "outputId": "5d0ba333-fa05-4580-a65b-eea2cdba4d7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7ae388071b10>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=datasets.Food101(root='./data',download=True,split= 'train')\n",
        "test_dataset=datasets.Food101(root='./data',download=True,split= 'test')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH3zGHwOv6Vl",
        "outputId": "7a61bebc-8147-40aa-b5ce-c68ed649bf5b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://data.vision.ee.ethz.ch/cvl/food-101.tar.gz to ./data/food-101.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4996278331/4996278331 [00:33<00:00, 151109660.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/food-101.tar.gz to ./data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.Food101(root='./data', download=True, split='train', transform=transforms.ToTensor())\n",
        "\n",
        "# Check the shape of the first image in the dataset to also confirm the number of channels and image size\n",
        "train_dataset[56][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2VirHJ9y6X-",
        "outputId": "f68e5afe-1ef2-44b7-9ba9-1b36480318a7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 384, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "                                    transforms.ToTensor(),  # Convert numpy array to tensor\n",
        "                                    transforms.Resize((256, 256)),transforms.RandomAffine( # Data Augmentation\n",
        "                                        degrees=(-5, 5), translate=(0, 0.05), scale=(0.9, 1.1)),\n",
        "                                        transforms.RandomResizedCrop((256, 256), scale=(0.75, 1))])\n",
        "\n",
        "val_transforms = transforms.Compose([transforms.ToTensor(),transforms.Resize((256, 256))])"
      ],
      "metadata": {
        "id": "TOyVRGSD8FyC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.Food101(root='./data', download=False, split='train', transform=train_transforms)\n",
        "val_dataset = datasets.Food101(root='./data', download=False, split='test', transform=val_transforms)\n"
      ],
      "metadata": {
        "id": "ejOjLoLKEojR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, )\n",
        "test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, )\n",
        "print(f\"There are {len(train_dataset)} train images and {len(val_dataset)} val images\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSr55-c4F2A4",
        "outputId": "226dd749-a1de-44c6-8225-897021813f0b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 75750 train images and 25250 val images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !pip install efficientnet_pytorch\n"
      ],
      "metadata": {
        "id": "YJrv-t6YcASy",
        "outputId": "bec9e673-1af0-4cae-e152-34391f7734da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet_pytorch) (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet_pytorch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet_pytorch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet_pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet_pytorch\n",
            "  Building wheel for efficientnet_pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet_pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16428 sha256=38b0aae4247111e856c7e1ad9b3772e6601485915099919ddcc4bdeb54eaed8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "Successfully built efficientnet_pytorch\n",
            "Installing collected packages: efficientnet_pytorch\n",
            "Successfully installed efficientnet_pytorch-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "model = EfficientNet.from_pretrained('efficientnet-b0')\n",
        "model._fc = nn.Linear(in_features=1280, out_features=101)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VtTIpxuCWR7A",
        "outputId": "0d78aa08-43c9-4e17-d511-8308cce20d2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b0-355c32eb.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b0-355c32eb.pth\n",
            "100%|██████████| 20.4M/20.4M [00:00<00:00, 267MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  !cat /proc/meminfo\n"
      ],
      "metadata": {
        "id": "oLxt4-2Yfyih"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create a neural net class\n",
        "# class Net(nn.Module):\n",
        "#     # Constructor\n",
        "#     def __init__(self, num_classes=101):\n",
        "#         super(Net, self).__init__()\n",
        "\n",
        "#         # Our images are RGB, so input channels = 3. We'll apply 32 filters in the first convolutional layer\n",
        "#         self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "#         self.bn1 = nn.BatchNorm2d(32)\n",
        "\n",
        "#         # A second convolutional layer takes 32 input channels, and generates 64 outputs\n",
        "#         self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "#         self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "#         # We'll apply max pooling with a kernel size of 2\n",
        "#         self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "#         # A third convolutional layer takes 64 input channels, and generates 128 outputs\n",
        "#         self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "#         self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "#         # A fourth convolutional layer takes 128 input channels, and generates 128 outputs\n",
        "#         self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "#         self.bn4 = nn.BatchNorm2d(256)\n",
        "\n",
        "#         self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "#         # A fifth convolutional layer takes 128 input channels, and generates 256 outputs\n",
        "#         self.conv5 = nn.Conv2d(in_channels=256, out_channels=8, kernel_size=3, stride=1, padding=1)\n",
        "#         self.bn5 = nn.BatchNorm2d(8)\n",
        "\n",
        "\n",
        "#          # We'll apply another max pooling with a kernel size of 2\n",
        "#         self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "#         # A drop layer deletes 20% of the features to help prevent overfitting\n",
        "#         self.drop = nn.Dropout2d(p=0.3)\n",
        "\n",
        "#         self.fc = nn.Linear(in_features=64 * 64 * 8, out_features=2048)\n",
        "#         self.fc2 = nn.Linear(in_features=2048, out_features=1024)\n",
        "#         self.fc3 = nn.Linear(in_features=1024, out_features=512)\n",
        "#         self.fc4 = nn.Linear(in_features=512, out_features=256)\n",
        "#         self.fc5 = nn.Linear(in_features=256, out_features=num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.bn1(self.conv1(x)))\n",
        "#         x = F.relu(self.bn2(self.conv2(x)))\n",
        "#         x = self.pool1(x)\n",
        "\n",
        "#         x = F.relu(self.bn3(self.conv3(x)))\n",
        "#         x = F.relu(self.bn4(self.conv4(x)))\n",
        "#         x = self.pool2(x)\n",
        "\n",
        "#         x = F.relu(self.bn5(self.conv5(x)))\n",
        "#         x = self.pool3(x)\n",
        "\n",
        "#         x = self.drop(x)\n",
        "\n",
        "#         # Flatten\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         # Feed to fully-connected layer to predict class\n",
        "#         x = self.fc(x)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.fc2(x)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.fc3(x)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.fc4(x)\n",
        "#         x = F.relu(x)\n",
        "#         x = self.fc5(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # Instantiate the model\n",
        "# model = Net()\n",
        "# print(\"CNN model class defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5b418mHGWuM",
        "outputId": "37bf0562-2e25-4d65-f2ba-80d741018f8b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN model class defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    print(\"Epoch:\", epoch)\n",
        "    # Process the images in batches\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Use the CPU or GPU as appropriate\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Reset the optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Push the data forward through the model layers\n",
        "        output = model(data)\n",
        "\n",
        "\n",
        "        # Get the loss\n",
        "        loss = loss_criteria(output, target)\n",
        "\n",
        "        # Keep a running total\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Backpropagate\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print metrics for every 10 batches so we see some progress\n",
        "        if batch_idx % 10 == 0:\n",
        "            print(f'Training set:{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader)} Loss:{loss.item()})')\n",
        "    # return average loss for the epoch\n",
        "    avg_loss = train_loss / (batch_idx+1)\n",
        "    print('Training set: Average loss: {:.6f}'.format(avg_loss))\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "Hdww7n-1nOEo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    # Switch the model to evaluation mode (so we don't backpropagate or drop)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    target_all = []\n",
        "    predicted_all = []\n",
        "    with torch.no_grad():\n",
        "        batch_count = 0\n",
        "        for data, target in test_loader:\n",
        "            batch_count += 1\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # Get the predicted classes for this batch\n",
        "            output = model(data)\n",
        "\n",
        "            # Calculate the loss for this batch\n",
        "            test_loss += loss_criteria(output, target).item()\n",
        "\n",
        "            # Calculate the accuracy for this batch\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            correct += torch.sum(target==predicted).item()\n",
        "            # Append scalar target and predicted values after moving into cpu\n",
        "            target_all.extend(target.cpu().numpy())\n",
        "            predicted_all.extend(predicted.cpu().numpy())\n",
        "\n",
        "\n",
        "    # Calculate the average loss and total accuracy for this epoch\n",
        "    avg_loss = test_loss/batch_count\n",
        "    print(f'Validation set: Average loss: {avg_loss}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset)}%)')\n",
        "    # Calculate Precision, Recall, and F1-score using scikit-learn functions\n",
        "    precision = round(precision_score(target_all, predicted_all, average='weighted'),2)\n",
        "    recall = round(recall_score(target_all, predicted_all, average='weighted'),2)\n",
        "    f1 = round(f1_score(target_all, predicted_all, average='weighted'),2)\n",
        "    precision2 = round(precision_score(target_all, predicted_all, average='micro'),2)\n",
        "    recall2 = round(recall_score(target_all, predicted_all, average='micro'),2)\n",
        "    f1_2 = round(f1_score(target_all, predicted_all, average='micro'),2)\n",
        "    accuracy = round((accuracy_score(target_all, predicted_all)*100),2)\n",
        "    # Printing the accuracy\n",
        "    print(f'Validation set: Average loss: {avg_loss},Accuracy: {accuracy}%, Weighted precision:{precision}, Weighted recall:{recall},Weighted f1:{f1}')\n",
        "    print(f'Micro precision:{precision2}, Micro recall:{recall2}, Micro f1:{f1_2}')\n",
        "   # return average loss for the epoch\n",
        "    return avg_loss,accuracy, precision, recall, f1\n",
        ""
      ],
      "metadata": {
        "id": "MxeUlxxzTYue"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ImRcwsu6Ef2x"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "if (torch.cuda.is_available()):\n",
        "    # if GPU available, use cuda (on a cpu, training will take a considerable length of time!)\n",
        "    device = \"cuda\"\n",
        "print('Training on', device)\n",
        "\n",
        "# Create an instance of the model class and allocate it to the device\n",
        "model = model.to(device)\n",
        "\n",
        "# Use an \"Adam\" optimizer to adjust weights\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Specify the loss criteria\n",
        "loss_criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "# Track metrics in these arrays\n",
        "epoch_nums = []\n",
        "training_loss = []\n",
        "validation_loss = []\n",
        "accuracy = []\n",
        "\n",
        "# Train over  epochs\n",
        "epochs = 20\n",
        "for epoch in range(1, epochs + 1):\n",
        "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
        "        test_loss = test(model, device, test_loader)\n",
        "        epoch_nums.append(epoch)\n",
        "        training_loss.append(train_loss)\n",
        "        validation_loss.append(test_loss[0])\n",
        "        accuracy.append(test_loss[1])\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R70R-PL8lN8v",
        "outputId": "063c8c95-36b1-4864-e8d0-a047493e80f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on cuda\n",
            "Epoch: 1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:4.6246538162231445)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:4.505284786224365)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:4.321415901184082)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:3.645277500152588)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:3.2854530811309814)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:3.1331374645233154)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:2.71018385887146)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:2.3399107456207275)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:2.50520920753479)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:2.285226821899414)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:2.2637126445770264)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:2.416304588317871)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:2.2177374362945557)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:2.287457227706909)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:1.8328560590744019)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:2.0900797843933105)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:2.3217616081237793)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:2.0073001384735107)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:1.6430165767669678)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:2.14103364944458)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:1.7267613410949707)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:1.59885573387146)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:2.026602268218994)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:1.842585563659668)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:1.933807373046875)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:1.9893697500228882)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:1.5544509887695312)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:1.500096321105957)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:1.3149304389953613)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:1.9297105073928833)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:1.6213215589523315)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:1.6522597074508667)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:1.5690754652023315)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:1.8577892780303955)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:1.5012584924697876)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:1.4783769845962524)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:1.4608678817749023)\n",
            "Training set:23680/75750 (31.25 Loss:1.6502535343170166)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:1.5142513513565063)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:1.5144493579864502)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:1.322221279144287)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:1.761075735092163)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:1.6993929147720337)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:1.2028120756149292)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:1.7238261699676514)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:1.4168890714645386)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:1.7226654291152954)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:1.58528733253479)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:1.5620909929275513)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:1.4202486276626587)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:1.274519920349121)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:1.3791025876998901)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:1.3159395456314087)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:1.6303610801696777)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:1.6614398956298828)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:1.4579983949661255)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:1.3963874578475952)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:1.5830035209655762)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:1.5189030170440674)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:1.5954787731170654)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:1.301653504371643)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:1.2280945777893066)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:1.2656149864196777)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:1.4048298597335815)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:1.1817545890808105)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:1.2505428791046143)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:1.345799446105957)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:1.7253217697143555)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:1.3198484182357788)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:1.5684045553207397)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:1.03770911693573)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:1.0097250938415527)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:1.4759072065353394)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:1.3386276960372925)\n",
            "Training set:47360/75750 (62.5 Loss:1.3125126361846924)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:1.807463526725769)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:1.5097744464874268)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.9347571730613708)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:1.526654601097107)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:1.323660969734192)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:1.5923945903778076)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:1.5158368349075317)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:1.2908687591552734)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:1.9151805639266968)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.90165776014328)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:1.6329749822616577)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:1.5846326351165771)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:1.2979487180709839)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:1.1403770446777344)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:1.4854457378387451)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:1.293060302734375)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:1.3403531312942505)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.9744225740432739)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:1.0677839517593384)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:1.060760498046875)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.8109214305877686)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.7794603109359741)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:1.1224390268325806)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:1.2119536399841309)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:1.5024422407150269)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:1.2147115468978882)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:1.1163718700408936)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:1.123049259185791)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:1.269856333732605)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.9915380477905273)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:1.5297168493270874)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:1.3970873355865479)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:1.0293315649032593)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:1.3682950735092163)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:1.1089974641799927)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:1.1178147792816162)\n",
            "Training set:71040/75750 (93.75 Loss:1.157578706741333)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:1.333276629447937)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:1.3059295415878296)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:1.1158356666564941)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.9581431746482849)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:1.3844550848007202)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:1.0457998514175415)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:1.3144611120224)\n",
            "Training set: Average loss: 1.618945\n",
            "Validation set: Average loss: 0.9932660971732834, Accuracy: 18325/25250 (72.57425742574257%)\n",
            "Validation set: Average loss: 0.9932660971732834,Accuracy: 72.57%, Weighted precision:0.76, Weighted recall:0.73,Weighted f1:0.73\n",
            "Micro precision:0.73, Micro recall:0.73, Micro f1:0.73\n",
            "Epoch: 2\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.7935667634010315)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.9395533204078674)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:1.1685951948165894)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:1.0007610321044922)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:1.0741828680038452)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:1.357496976852417)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.936224639415741)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.984021008014679)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.9726121425628662)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.8747307062149048)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.5767560601234436)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.9950500130653381)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:1.2806636095046997)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.9897451996803284)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:1.2458001375198364)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:1.2535775899887085)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.786030650138855)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:1.093242883682251)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.8630704879760742)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.7619467377662659)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:1.0903156995773315)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.8554897308349609)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:1.0017666816711426)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:1.09128999710083)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.6895772218704224)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:1.2920721769332886)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:1.3956609964370728)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:1.153363823890686)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:1.567188024520874)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:1.0204051733016968)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:1.0074940919876099)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:1.0044655799865723)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.8645820021629333)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:1.185296654701233)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:1.2685226202011108)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:1.2195560932159424)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:1.010890245437622)\n",
            "Training set:23680/75750 (31.25 Loss:1.034887671470642)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:1.0747958421707153)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:1.0268967151641846)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.954732358455658)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:1.1438039541244507)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:1.1480242013931274)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.6917292475700378)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:1.0531812906265259)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:1.4251478910446167)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:1.1416079998016357)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.806603729724884)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:1.1175884008407593)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.9375420212745667)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.8931223750114441)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:1.1879348754882812)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:1.0701649188995361)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:1.0967819690704346)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:1.011034369468689)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.9763451218605042)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:1.0575438737869263)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.885534942150116)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.6602824926376343)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.8196349740028381)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:1.1276718378067017)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:1.3521239757537842)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.6945933103561401)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:1.0503253936767578)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.9986193180084229)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.7809281349182129)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:1.149294376373291)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.9752736687660217)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:1.0976425409317017)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:1.5209150314331055)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:1.1912096738815308)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.8370766043663025)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.9278125762939453)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.9539045095443726)\n",
            "Training set:47360/75750 (62.5 Loss:0.9279544949531555)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:1.088883399963379)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:1.1402870416641235)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:1.130242943763733)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.9172342419624329)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.9862937927246094)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:1.1028674840927124)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:1.2920671701431274)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:1.0181574821472168)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:1.1384525299072266)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:1.224678874015808)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.8382729887962341)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.7937174439430237)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.849422037601471)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.8956608176231384)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.9373006224632263)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:1.025017499923706)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.8445005416870117)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:1.0922514200210571)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.9104917645454407)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:1.3274447917938232)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.9453948736190796)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.925415575504303)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:1.1650104522705078)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.9636204242706299)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:1.0426558256149292)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.9835531711578369)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:1.0556941032409668)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:1.0654401779174805)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.9680975079536438)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.8093465566635132)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.9956504106521606)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:1.1955761909484863)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.9398071765899658)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:1.0757153034210205)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:1.220262050628662)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.9276509284973145)\n",
            "Training set:71040/75750 (93.75 Loss:1.118364691734314)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.6589831709861755)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:1.0654540061950684)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.9198928475379944)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.902321457862854)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:1.1669684648513794)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.5794340968132019)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.9012706875801086)\n",
            "Training set: Average loss: 1.023675\n",
            "Validation set: Average loss: 0.8628487665749404, Accuracy: 19190/25250 (76.0%)\n",
            "Validation set: Average loss: 0.8628487665749404,Accuracy: 76.0%, Weighted precision:0.79, Weighted recall:0.76,Weighted f1:0.76\n",
            "Micro precision:0.76, Micro recall:0.76, Micro f1:0.76\n",
            "Epoch: 3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.8071209192276001)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.8847696185112)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.7932879328727722)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.8217309713363647)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.5609041452407837)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.8273100852966309)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.9474755525588989)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.7427546381950378)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.7433148622512817)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:1.1394627094268799)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.8457704782485962)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:1.0041959285736084)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.8285696506500244)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.5320795774459839)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.6848897337913513)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.7110374569892883)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:1.1572105884552002)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:1.0775270462036133)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.644401490688324)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.9014339447021484)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.47149962186813354)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.566703200340271)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.8052898645401001)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:1.0836524963378906)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.4751329720020294)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:1.1456365585327148)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.740760862827301)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.9781972169876099)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.9571852684020996)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.5826759338378906)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.7401362657546997)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.8724616765975952)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.7318969964981079)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.5093842148780823)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.8757201433181763)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.9687310457229614)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:1.2502769231796265)\n",
            "Training set:23680/75750 (31.25 Loss:0.6106631755828857)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.7235567569732666)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.8428018093109131)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.8198133707046509)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.9015170931816101)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.7752174735069275)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.7859593629837036)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.7928513288497925)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.9378876686096191)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.8687721490859985)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.9673166275024414)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:1.0222816467285156)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.6519272327423096)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:1.3069407939910889)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.5381898880004883)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:1.1307353973388672)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.779775857925415)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.8248268961906433)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.6777060627937317)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.7531251311302185)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.7250418663024902)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.45927613973617554)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.8509316444396973)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.5907738208770752)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.9087887406349182)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.8125918507575989)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.5878358483314514)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:1.0080333948135376)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.9959720373153687)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.5068491101264954)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:1.077663779258728)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.8624948859214783)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.9160000681877136)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.9936524033546448)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:1.20289945602417)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.8067727088928223)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:1.0479098558425903)\n",
            "Training set:47360/75750 (62.5 Loss:0.9292776584625244)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.8939847946166992)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.9149956703186035)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.9622136950492859)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.6618096828460693)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.9754385948181152)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.9001497030258179)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.4448411762714386)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.931880533695221)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:1.2536804676055908)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:1.0008511543273926)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.9191264510154724)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.962988555431366)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.8606760501861572)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.9681086540222168)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.9483639001846313)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.891295850276947)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.8119563460350037)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.7330799102783203)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.520878791809082)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.5779808759689331)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.7657577991485596)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.6144332885742188)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.6145880818367004)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.9950522780418396)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.8221086859703064)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:1.0042933225631714)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.43705421686172485)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.846100926399231)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.96111661195755)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.7666126489639282)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.7471845149993896)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:1.0915075540542603)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:1.0408949851989746)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.7764934301376343)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:1.1402487754821777)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:1.1140475273132324)\n",
            "Training set:71040/75750 (93.75 Loss:0.98265141248703)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.9861162900924683)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:1.016952395439148)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.9773460030555725)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:1.0488057136535645)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.7310899496078491)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:1.108100175857544)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.8992797136306763)\n",
            "Training set: Average loss: 0.851464\n",
            "Validation set: Average loss: 0.8884109982772719, Accuracy: 19081/25250 (75.56831683168316%)\n",
            "Validation set: Average loss: 0.8884109982772719,Accuracy: 75.57%, Weighted precision:0.78, Weighted recall:0.76,Weighted f1:0.76\n",
            "Micro precision:0.76, Micro recall:0.76, Micro f1:0.76\n",
            "Epoch: 4\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.7717309594154358)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.6947535276412964)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.6467095613479614)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.823436439037323)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.614844560623169)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.7271544337272644)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.6888834238052368)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.5527493357658386)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.7096042037010193)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.7234097719192505)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.3786626160144806)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.9453151822090149)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.682316243648529)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.6622116565704346)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.5051313638687134)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.46326589584350586)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.49667778611183167)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.7153839468955994)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.8100053668022156)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:1.0247327089309692)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.5611864328384399)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.7707311511039734)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.6869900226593018)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:1.004509687423706)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.8063470125198364)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.4361172616481781)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.7794338464736938)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.7283830642700195)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.9902722835540771)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.6770555377006531)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.9237479567527771)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.6636444926261902)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.6489103436470032)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.6869556903839111)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.5418411493301392)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.6459077596664429)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.7640711665153503)\n",
            "Training set:23680/75750 (31.25 Loss:0.7381575107574463)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.8771340250968933)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.8764784336090088)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.6428893804550171)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.592715859413147)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.9831812381744385)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.7406954169273376)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.657670795917511)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:1.073877215385437)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.6515538096427917)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.6421011686325073)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.879382312297821)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.5527873635292053)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:1.1911002397537231)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.6827809810638428)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.7459928393363953)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.526199221611023)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.8210262060165405)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:1.041778564453125)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.7216734290122986)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.7008268237113953)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.6864494681358337)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.966332733631134)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.5669971108436584)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.8819267153739929)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.8435855507850647)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.7629109621047974)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:1.3670766353607178)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:1.170221209526062)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.7685629725456238)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.6672561764717102)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:1.1697579622268677)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.7245575785636902)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.7891982793807983)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.66087406873703)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.8091246485710144)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.6848026514053345)\n",
            "Training set:47360/75750 (62.5 Loss:0.7496935129165649)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.5938369035720825)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.6967777609825134)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.8129607439041138)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.7133503556251526)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.7510606050491333)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.748039722442627)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.530204713344574)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.9144227504730225)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.44812825322151184)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.404666006565094)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.5486800074577332)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.9596391916275024)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.6470866799354553)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.5396934151649475)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.6823917031288147)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:1.1438347101211548)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.7251158952713013)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.8945838212966919)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.957624077796936)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.5745848417282104)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.9707162380218506)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.675979495048523)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.9060165286064148)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.6531752347946167)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.535666823387146)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.48899975419044495)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.5608002543449402)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.45059365034103394)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.6745181083679199)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.7738334536552429)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.3970259726047516)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.691832423210144)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.5771574974060059)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.6699358820915222)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.7802367806434631)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.8056314587593079)\n",
            "Training set:71040/75750 (93.75 Loss:0.6524529457092285)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.8235880136489868)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.8376559615135193)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.822556734085083)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.7328653931617737)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.6024371981620789)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:1.0019588470458984)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:1.0176345109939575)\n",
            "Training set: Average loss: 0.742244\n",
            "Validation set: Average loss: 0.8418531873487408, Accuracy: 19552/25250 (77.43366336633663%)\n",
            "Validation set: Average loss: 0.8418531873487408,Accuracy: 77.43%, Weighted precision:0.79, Weighted recall:0.77,Weighted f1:0.77\n",
            "Micro precision:0.77, Micro recall:0.77, Micro f1:0.77\n",
            "Epoch: 5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.5941736102104187)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.376156210899353)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.6668930053710938)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.3567880392074585)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.679553210735321)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.574278712272644)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.8021941781044006)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.6420169472694397)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.6714114546775818)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.5529265999794006)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.44145792722702026)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.504546046257019)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.47421959042549133)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.45767033100128174)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.4020560681819916)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.6125343441963196)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.5727473497390747)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.5804914236068726)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.611962080001831)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.7612461447715759)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.42105910181999207)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.6432926654815674)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.9085942506790161)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.7616958618164062)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.6732101440429688)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.3531240224838257)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.9547607898712158)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.4230704605579376)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.44333910942077637)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:1.1507573127746582)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.4900361895561218)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.32591620087623596)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.9637022614479065)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.6111887693405151)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.48704954981803894)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.8484193682670593)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.6177983283996582)\n",
            "Training set:23680/75750 (31.25 Loss:0.8427170515060425)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.664915919303894)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.7337786555290222)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.7064690589904785)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.7700563669204712)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.5187149047851562)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.7343781590461731)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.6242849826812744)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.9727368950843811)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.570898711681366)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.6214333772659302)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.5200965404510498)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.44880911707878113)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.7810910940170288)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.4279308617115021)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.6446930170059204)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.8546682596206665)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.7083242535591125)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.649471640586853)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.529988169670105)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.5761281251907349)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.5521949529647827)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.8203791975975037)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.8040530681610107)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.7178003191947937)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.5372950434684753)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.5252339839935303)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.6123709082603455)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.7654287219047546)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.8315115571022034)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.4936072826385498)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.725135087966919)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.7622642517089844)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.5193924903869629)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.7601171731948853)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.8280330896377563)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.6884123682975769)\n",
            "Training set:47360/75750 (62.5 Loss:0.6543928384780884)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.5697951316833496)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.4666810631752014)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.6317782402038574)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.4841518700122833)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.7875968217849731)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.6020179986953735)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.8433163166046143)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.8765515685081482)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.764119029045105)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.8698098063468933)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.8887720108032227)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.2994762361049652)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.5128978490829468)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.6720248460769653)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.8182851672172546)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.5889047980308533)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.4788321554660797)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.758847177028656)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.441665381193161)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.5459772348403931)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.5189964175224304)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.7681512832641602)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.628184974193573)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.6083613038063049)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.49000200629234314)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.4635295867919922)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.5539686679840088)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:1.09980046749115)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.5134766101837158)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.582442581653595)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.9349502921104431)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.47322511672973633)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.8911649584770203)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.6056503653526306)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.6513743996620178)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.7816945910453796)\n",
            "Training set:71040/75750 (93.75 Loss:0.6045048832893372)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.5490562319755554)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.588403582572937)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.6540369987487793)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.9783492684364319)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.813101589679718)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.43011870980262756)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.4937613904476166)\n",
            "Training set: Average loss: 0.648490\n",
            "Validation set: Average loss: 0.8152061269818982, Accuracy: 19774/25250 (78.31287128712871%)\n",
            "Validation set: Average loss: 0.8152061269818982,Accuracy: 78.31%, Weighted precision:0.8, Weighted recall:0.78,Weighted f1:0.78\n",
            "Micro precision:0.78, Micro recall:0.78, Micro f1:0.78\n",
            "Epoch: 6\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.42223355174064636)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.46422749757766724)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.3757304549217224)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.34916427731513977)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.5822562575340271)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.5864933729171753)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.7560226917266846)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.5517441034317017)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.5106896758079529)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.566476047039032)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.41560259461402893)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.3335030972957611)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.5680991411209106)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.4309203624725342)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.5722525119781494)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.6224448084831238)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.6147325038909912)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.7142366170883179)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.7164692282676697)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.5673204660415649)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.5328118205070496)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.22425387799739838)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.7348079681396484)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.35771167278289795)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.2871190905570984)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.6480324268341064)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.2414294332265854)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.6201505064964294)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.7509158849716187)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.3959645926952362)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.4737081527709961)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.5392975211143494)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.28300994634628296)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.7172013521194458)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.48955515027046204)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.4368336796760559)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.6707479953765869)\n",
            "Training set:23680/75750 (31.25 Loss:0.5989795923233032)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.6656805872917175)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.4426131546497345)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.8019424676895142)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.7159813642501831)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.46093323826789856)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.3621949553489685)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.6796744465827942)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.526435911655426)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.8267422318458557)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.5627687573432922)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.5781483054161072)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.5460734367370605)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.256050705909729)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.5076847076416016)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.5230314135551453)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.5891687870025635)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.7895963788032532)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.7356463670730591)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.6070109605789185)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.5756505727767944)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.8083131313323975)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.6348959803581238)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.5391411185264587)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.42770808935165405)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.5450121164321899)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.8431306481361389)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.4471488893032074)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.3676559031009674)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.6236436367034912)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.819973886013031)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.6724964380264282)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.5494270324707031)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.7527941465377808)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.7418951392173767)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.6651216149330139)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.6711522340774536)\n",
            "Training set:47360/75750 (62.5 Loss:0.5486205220222473)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.7962208986282349)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.6580700874328613)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.39376649260520935)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.5632790327072144)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.4763372242450714)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.4796343147754669)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.5121650695800781)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.6904852390289307)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.39734071493148804)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.5818532705307007)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.35642582178115845)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.5850775837898254)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.37033140659332275)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.5836030840873718)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.5727028846740723)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.32276999950408936)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.5550030469894409)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.5165076851844788)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.8959988355636597)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.6184155344963074)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.5937437415122986)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.4886770248413086)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.7401535511016846)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.5026593208312988)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.5241450071334839)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.5930755138397217)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.5695949196815491)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.46575823426246643)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.6856793165206909)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.3851349651813507)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.7958527207374573)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.5030382871627808)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.5848903656005859)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.819168746471405)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.6126477122306824)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.7774354219436646)\n",
            "Training set:71040/75750 (93.75 Loss:0.5180371999740601)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.6516671180725098)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.4459899067878723)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.5706289410591125)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.3946021497249603)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.49776485562324524)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.5447515249252319)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.5045031309127808)\n",
            "Training set: Average loss: 0.577297\n",
            "Validation set: Average loss: 0.8225061440816785, Accuracy: 19672/25250 (77.9089108910891%)\n",
            "Validation set: Average loss: 0.8225061440816785,Accuracy: 77.91%, Weighted precision:0.79, Weighted recall:0.78,Weighted f1:0.78\n",
            "Micro precision:0.78, Micro recall:0.78, Micro f1:0.78\n",
            "Epoch: 7\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.47042691707611084)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.24292437732219696)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.29905733466148376)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.3021697998046875)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.3318565785884857)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.32854852080345154)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.44153672456741333)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.3837011158466339)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.3465505838394165)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.43165284395217896)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.4734838008880615)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.7796753644943237)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.5332981944084167)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.5768172740936279)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.3259907066822052)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.3192363679409027)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.43267253041267395)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.4458828866481781)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.3760496973991394)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.2613292932510376)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.41861218214035034)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.6675407290458679)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.545293927192688)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.39645835757255554)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.3809380531311035)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.4167245626449585)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.3180462718009949)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.6328249573707581)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.5748531222343445)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.33476778864860535)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.6943225264549255)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.5820577144622803)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.3710503578186035)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.6546850204467773)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.5429664254188538)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.3474591374397278)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.3139328360557556)\n",
            "Training set:23680/75750 (31.25 Loss:0.37549465894699097)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.4366166293621063)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.6992151141166687)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.5301249623298645)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.5223115682601929)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.43454423546791077)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.4821476638317108)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.8779255151748657)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.6013470888137817)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.5973830819129944)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.5415379405021667)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.6984533071517944)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.3781278431415558)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.6278246641159058)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.41760504245758057)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.36176127195358276)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.65431809425354)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.7238170504570007)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.4770825207233429)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.3642909526824951)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.7584444284439087)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.617357611656189)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.5021902918815613)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.4218457341194153)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.5621501207351685)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.7446854114532471)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.5599208474159241)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.5171764492988586)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.35600191354751587)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.4522920548915863)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.4800450801849365)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.43188729882240295)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.26346081495285034)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.2964586615562439)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.5402844548225403)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.593315839767456)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.4851098358631134)\n",
            "Training set:47360/75750 (62.5 Loss:1.0371911525726318)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.39850884675979614)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.6664371490478516)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.6062361001968384)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.5033320188522339)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.622229814529419)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.5259264707565308)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.6239769458770752)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.4067084789276123)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.7397481203079224)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.6583012342453003)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.45004913210868835)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.6701231002807617)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.5518925786018372)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.6821515560150146)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.3154793679714203)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.808944582939148)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.3360850512981415)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.6390026211738586)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.7013003826141357)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.3588601350784302)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.37612470984458923)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.48788395524024963)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.5935992002487183)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.52866131067276)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.6998094320297241)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.17960631847381592)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.6441516280174255)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.6216413378715515)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.7110082507133484)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.6164474487304688)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.6243138313293457)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.691278874874115)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.3703800439834595)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.42164093255996704)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.3235381245613098)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.4010987877845764)\n",
            "Training set:71040/75750 (93.75 Loss:0.5622280240058899)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.7641026973724365)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.6867126226425171)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.6143604516983032)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.7407239675521851)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.6713608503341675)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.6296652555465698)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.6698117852210999)\n",
            "Training set: Average loss: 0.514300\n",
            "Validation set: Average loss: 0.8020186743875848, Accuracy: 20087/25250 (79.55247524752475%)\n",
            "Validation set: Average loss: 0.8020186743875848,Accuracy: 79.55%, Weighted precision:0.81, Weighted recall:0.8,Weighted f1:0.8\n",
            "Micro precision:0.8, Micro recall:0.8, Micro f1:0.8\n",
            "Epoch: 8\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.49450480937957764)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.21787171065807343)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.4618918299674988)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.3728589415550232)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.33862265944480896)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.2353004664182663)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.3194260895252228)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.388226717710495)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.1442684531211853)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.16763028502464294)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.4028717875480652)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.775705099105835)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.3428119122982025)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.4155401885509491)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.47865769267082214)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.4991456866264343)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.4680067002773285)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.32197070121765137)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.33768731355667114)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.3403301537036896)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.38470813632011414)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.6048734188079834)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.39366790652275085)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.432693213224411)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.5234272480010986)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.18514569103717804)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.3890291154384613)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.36697450280189514)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.45450496673583984)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.37341272830963135)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.32591450214385986)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.40339377522468567)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.6091842651367188)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.5675076246261597)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.44088730216026306)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.33441850543022156)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.4236427843570709)\n",
            "Training set:23680/75750 (31.25 Loss:0.39772331714630127)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.4529419541358948)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.526947021484375)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.47116759419441223)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.3206990957260132)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.38900113105773926)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.4626772701740265)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.31642472743988037)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.7071503400802612)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.31176865100860596)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.5068931579589844)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.47773417830467224)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.3742789030075073)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.6013973355293274)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.3952414095401764)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.31628522276878357)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.3484695255756378)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.4956704080104828)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.6009514331817627)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.4867568016052246)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.4668329656124115)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.7049981951713562)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.4285304546356201)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.4304390847682953)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.33422359824180603)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.6051512956619263)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.6284036040306091)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.4565800130367279)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.4523099958896637)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.42390531301498413)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.7856401801109314)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.381335973739624)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.49573132395744324)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.5769250392913818)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.44996628165245056)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.5125048160552979)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.43227866291999817)\n",
            "Training set:47360/75750 (62.5 Loss:0.3116653263568878)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.43092605471611023)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.48868241906166077)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.5009310841560364)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.7697261571884155)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.5236498713493347)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.8454887270927429)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.26579657196998596)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.37777796387672424)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.6575905084609985)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.5435207486152649)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.23174259066581726)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.32870739698410034)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.6574360728263855)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.514255166053772)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.4381904900074005)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.4923190176486969)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.32577890157699585)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.39863476157188416)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.5245932340621948)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.6639388799667358)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.49115291237831116)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.5827411413192749)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.7209604978561401)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.4587792456150055)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.6083337068557739)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.5032688975334167)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.7669997215270996)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.4583008885383606)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.5255568623542786)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.605141818523407)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.47982537746429443)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.7573415637016296)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.4488026797771454)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.41286903619766235)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.4592767357826233)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.6363745927810669)\n",
            "Training set:71040/75750 (93.75 Loss:0.6077197194099426)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.5257078409194946)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.43167686462402344)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.3295760154724121)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.48250657320022583)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.47354522347450256)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.6644640564918518)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.5643417835235596)\n",
            "Training set: Average loss: 0.461122\n",
            "Validation set: Average loss: 0.8112959444805791, Accuracy: 20035/25250 (79.34653465346534%)\n",
            "Validation set: Average loss: 0.8112959444805791,Accuracy: 79.35%, Weighted precision:0.81, Weighted recall:0.79,Weighted f1:0.79\n",
            "Micro precision:0.79, Micro recall:0.79, Micro f1:0.79\n",
            "Epoch: 9\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.36352360248565674)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.19088861346244812)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.34127169847488403)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.23491419851779938)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.3611191213130951)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.3269377052783966)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.5166139006614685)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.37964028120040894)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.3048149347305298)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.37948447465896606)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.39564669132232666)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.5684572458267212)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.3503926694393158)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.4050257205963135)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.236723855137825)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.1691669076681137)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.3652925491333008)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.3506414294242859)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.48254337906837463)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.6892396211624146)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.30004990100860596)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.2915527820587158)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.3940083086490631)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.2574490010738373)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.519694447517395)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.457388311624527)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.5525033473968506)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.3332323729991913)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.4369259476661682)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.4465779960155487)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.43024274706840515)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.48772451281547546)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.4927288889884949)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.38400599360466003)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.6196674704551697)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.2954779863357544)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.5012367963790894)\n",
            "Training set:23680/75750 (31.25 Loss:0.42137300968170166)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.39760634303092957)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.3436668813228607)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.4697471261024475)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.41621991991996765)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.3149532675743103)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.3491668701171875)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.2627698481082916)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.33656495809555054)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.36306267976760864)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.19681395590305328)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.28824782371520996)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.3185119926929474)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.2720354199409485)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.22629256546497345)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.7222951650619507)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.4587290585041046)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.32111746072769165)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.3964177966117859)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.32807737588882446)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.46136337518692017)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.37314650416374207)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.3925762176513672)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.5328935980796814)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.4385851323604584)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.5218992233276367)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.6916366219520569)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.16708925366401672)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.3488784432411194)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.4425908029079437)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.27226999402046204)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.4359947144985199)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.5003221035003662)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.6855552196502686)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.3939487636089325)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.3180467188358307)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.42056038975715637)\n",
            "Training set:47360/75750 (62.5 Loss:0.39738699793815613)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.42996716499328613)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.3624532222747803)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.41454440355300903)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.5492323637008667)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.39029109477996826)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.3874450922012329)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.20691044628620148)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.3810373842716217)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.501309335231781)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.4690486490726471)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.5049861073493958)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.5575799942016602)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.33531540632247925)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.4729263186454773)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.43912747502326965)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.4543501138687134)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.3343474864959717)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.4976874589920044)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.5731250047683716)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.4207570254802704)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.6622654795646667)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.6997575163841248)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.4456203877925873)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.4606197476387024)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.7519903182983398)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.346212774515152)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.49965187907218933)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.33812084794044495)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.27078235149383545)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.24730071425437927)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.3386182188987732)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.6475766897201538)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.30987417697906494)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.6039135456085205)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.4919939339160919)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.4225505590438843)\n",
            "Training set:71040/75750 (93.75 Loss:0.4419913589954376)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.5713644623756409)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.42687463760375977)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.3082221448421478)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.3718419075012207)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.5843573212623596)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.3589106500148773)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.740563154220581)\n",
            "Training set: Average loss: 0.415947\n",
            "Validation set: Average loss: 0.8389165887161146, Accuracy: 19987/25250 (79.15643564356435%)\n",
            "Validation set: Average loss: 0.8389165887161146,Accuracy: 79.16%, Weighted precision:0.8, Weighted recall:0.79,Weighted f1:0.79\n",
            "Micro precision:0.79, Micro recall:0.79, Micro f1:0.79\n",
            "Epoch: 10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.3574300706386566)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.3916820287704468)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.3010019063949585)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.4622932970523834)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.23662744462490082)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.20082706212997437)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.32205185294151306)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.2299034297466278)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.32247868180274963)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.3273940682411194)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.19078126549720764)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.2607627213001251)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.2403179556131363)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.3098987638950348)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.41680464148521423)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.3104456961154938)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.5724692940711975)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.33303916454315186)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.3014993667602539)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.24066974222660065)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.40211665630340576)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.3087456226348877)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.22365950047969818)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.4067710340023041)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.26525965332984924)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.31213390827178955)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.28534242510795593)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.4112755358219147)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.2936970889568329)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.3127208352088928)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.2549852132797241)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.12744702398777008)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.1896926611661911)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.47502437233924866)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.5515055656433105)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.47079479694366455)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.4608396887779236)\n",
            "Training set:23680/75750 (31.25 Loss:0.46010622382164)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.4130892753601074)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.235739603638649)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.373755544424057)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.46141281723976135)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.3085029125213623)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.2973402738571167)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.3062295913696289)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.49456071853637695)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.2314986288547516)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.31859830021858215)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.10625146329402924)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.30215907096862793)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.42164361476898193)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.4567102789878845)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.3820666968822479)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.2528897523880005)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.16585323214530945)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.3970448076725006)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.6563518643379211)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.3259633779525757)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.6237971186637878)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.49213841557502747)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.36347243189811707)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.16613151133060455)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.46340882778167725)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.24879129230976105)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.1835600584745407)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.39417564868927)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.5047466158866882)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.2857811748981476)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.28661686182022095)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.3927537202835083)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.5680274367332458)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.384105920791626)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.264265775680542)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.4288860559463501)\n",
            "Training set:47360/75750 (62.5 Loss:0.16351447999477386)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.34993791580200195)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.6630110740661621)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.30199676752090454)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.4832024872303009)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.46878981590270996)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.2382199466228485)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.4796627163887024)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.394277423620224)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.46454060077667236)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.3532509207725525)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.18730054795742035)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.5203719139099121)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.4398758113384247)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.4767066538333893)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.21258516609668732)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.36145588755607605)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.28131887316703796)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.4686259627342224)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.45837751030921936)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.5060551762580872)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.5859311819076538)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.6430840492248535)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.24466964602470398)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.27707648277282715)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.42173489928245544)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.4422859847545624)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.5862065553665161)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.4456790089607239)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.35668107867240906)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.1518884003162384)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.3317207098007202)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.2209341824054718)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.43294757604599)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.4129105806350708)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.36555108428001404)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.4779956340789795)\n",
            "Training set:71040/75750 (93.75 Loss:0.45113393664360046)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.4518141746520996)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.4031998813152313)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.44506457448005676)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.6707841753959656)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.2700258493423462)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.30549156665802)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.4113630950450897)\n",
            "Training set: Average loss: 0.378462\n",
            "Validation set: Average loss: 0.8288007771712728, Accuracy: 20185/25250 (79.94059405940594%)\n",
            "Validation set: Average loss: 0.8288007771712728,Accuracy: 79.94%, Weighted precision:0.81, Weighted recall:0.8,Weighted f1:0.8\n",
            "Micro precision:0.8, Micro recall:0.8, Micro f1:0.8\n",
            "Epoch: 11\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.2764996290206909)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.2699645459651947)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.21747611463069916)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.2563689947128296)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.24229544401168823)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.22296370565891266)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.2050856053829193)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.3058190643787384)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.4239136874675751)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.25944945216178894)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.25646063685417175)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.20435622334480286)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.15355458855628967)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.18683351576328278)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.21527807414531708)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.19447889924049377)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.35535144805908203)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.31066134572029114)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.21629220247268677)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.33549970388412476)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.2733840346336365)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.34383291006088257)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.44028380513191223)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.2995567321777344)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.33398059010505676)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.5025761127471924)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.3628828823566437)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.533416748046875)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.12250502407550812)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.44458937644958496)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.22525125741958618)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.23234254121780396)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.4726864993572235)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.33487468957901)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.18258549273014069)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.3900570273399353)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.3391854166984558)\n",
            "Training set:23680/75750 (31.25 Loss:0.38311585783958435)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.3632485866546631)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.2011151909828186)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.36652904748916626)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.2659359276294708)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.33070823550224304)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.363797664642334)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.34854158759117126)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.30468904972076416)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.38381657004356384)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.3544726073741913)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.1787593960762024)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.1887773722410202)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.2767578065395355)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.2760885953903198)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.37617602944374084)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.45120513439178467)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.4782745838165283)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.4207700192928314)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.17574553191661835)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.3347465395927429)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.3745616376399994)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.3798742890357971)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.2531667649745941)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.4134882390499115)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.3054434657096863)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.26055482029914856)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.2967170774936676)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.5366701483726501)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.3341904878616333)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.23993325233459473)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.2318408340215683)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.46011415123939514)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.31446734070777893)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.5571157336235046)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.2736286520957947)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.2281312346458435)\n",
            "Training set:47360/75750 (62.5 Loss:0.29738113284111023)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.4360053539276123)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.34435510635375977)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.3539731204509735)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.40612363815307617)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.2076808661222458)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.2516227066516876)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.36388471722602844)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.4644545614719391)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.33735108375549316)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.2538454234600067)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.30223333835601807)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.484301894903183)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.33638280630111694)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.38781458139419556)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.17476952075958252)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.4724425971508026)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.2912653088569641)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.385117769241333)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.4334413409233093)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.18760322034358978)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.3237417936325073)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.44738709926605225)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.5140348672866821)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.2966999113559723)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.27663686871528625)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.4853288531303406)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.4798182249069214)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.36417585611343384)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.11398879438638687)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.2039262354373932)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.5517854690551758)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.5464710593223572)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.48437193036079407)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.331125944852829)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.43100354075431824)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.5148120522499084)\n",
            "Training set:71040/75750 (93.75 Loss:0.4622272849082947)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.47078412771224976)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.20930564403533936)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.29847875237464905)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.3058372139930725)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.30068978667259216)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.648658812046051)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.5703060030937195)\n",
            "Training set: Average loss: 0.339048\n",
            "Validation set: Average loss: 1.1243255101887013, Accuracy: 18863/25250 (74.7049504950495%)\n",
            "Validation set: Average loss: 1.1243255101887013,Accuracy: 74.7%, Weighted precision:0.8, Weighted recall:0.75,Weighted f1:0.76\n",
            "Micro precision:0.75, Micro recall:0.75, Micro f1:0.75\n",
            "Epoch: 12\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.3460066318511963)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.20782972872257233)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.12261264026165009)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.2810172736644745)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.4053574502468109)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.17462964355945587)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.1753799170255661)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.5648687481880188)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.16158227622509003)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.20768550038337708)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.20166410505771637)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.4401495158672333)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.22295677661895752)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.18031610548496246)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.28440648317337036)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.0922856554389)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.39626792073249817)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.5822829008102417)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.37812334299087524)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.30196458101272583)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.2963462173938751)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.2918851375579834)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.272823303937912)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.16033874452114105)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.36718958616256714)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.228801429271698)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.13832035660743713)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.32667046785354614)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.18540748953819275)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.21713963150978088)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.17199654877185822)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.1894540786743164)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.1931883692741394)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.31822726130485535)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.5604467391967773)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.37951478362083435)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.2695406973361969)\n",
            "Training set:23680/75750 (31.25 Loss:0.3956478238105774)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.2672445774078369)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.29340219497680664)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.22232505679130554)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.23765842616558075)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.3043273687362671)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.2525082230567932)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.5335714817047119)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.2664194107055664)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.22113484144210815)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.34002429246902466)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.49692100286483765)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.5498520731925964)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.15268553793430328)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.3424120843410492)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.3028213083744049)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.1352817714214325)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.17896312475204468)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.33526667952537537)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.31470128893852234)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.16643567383289337)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.446328341960907)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.29827162623405457)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.40915265679359436)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.3299553394317627)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.4891974925994873)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.24800986051559448)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.2569841742515564)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.1831967830657959)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.3484814167022705)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.4040621817111969)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.19884613156318665)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.29256051778793335)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.3240741789340973)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.26080411672592163)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.2844367027282715)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.3546285331249237)\n",
            "Training set:47360/75750 (62.5 Loss:0.4723469913005829)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.41213542222976685)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.24047604203224182)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.5626556277275085)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.2551622688770294)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.28295642137527466)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.3468270003795624)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.40400922298431396)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.35293298959732056)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.29551196098327637)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.4247986376285553)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.4856302738189697)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.3501927852630615)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.4303596317768097)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.8276481032371521)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.4845673441886902)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.42224130034446716)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.39764994382858276)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.26789334416389465)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.33558863401412964)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.175681471824646)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.29116272926330566)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.2899048328399658)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.3727138042449951)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.8839529156684875)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.4260883033275604)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.09418094158172607)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.2508113384246826)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.21411122381687164)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.33060041069984436)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.362697571516037)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.4076409637928009)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.26881906390190125)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.41040462255477905)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.2761150002479553)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.6087282299995422)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.328175812959671)\n",
            "Training set:71040/75750 (93.75 Loss:0.4980184733867645)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.8350785970687866)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.3817955255508423)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.20983442664146423)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.40173035860061646)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.2038159966468811)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.3670646548271179)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.3155229687690735)\n",
            "Training set: Average loss: 0.320991\n",
            "Validation set: Average loss: 0.8571929350730564, Accuracy: 20244/25250 (80.17425742574257%)\n",
            "Validation set: Average loss: 0.8571929350730564,Accuracy: 80.17%, Weighted precision:0.81, Weighted recall:0.8,Weighted f1:0.8\n",
            "Micro precision:0.8, Micro recall:0.8, Micro f1:0.8\n",
            "Epoch: 13\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.18270529806613922)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.15875934064388275)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.16764108836650848)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.16950975358486176)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.31830063462257385)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.17893928289413452)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.37203437089920044)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.23799604177474976)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.14336231350898743)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.23056915402412415)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.12623010575771332)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.24034114181995392)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.30624011158943176)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.10154004395008087)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.30744731426239014)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.3703393042087555)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.15292944014072418)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.09206480532884598)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.31189775466918945)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.17803406715393066)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.3265615999698639)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.3300829529762268)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.37111225724220276)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.30237165093421936)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.12582369148731232)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.2092055082321167)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.2132311463356018)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.24511252343654633)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.13034670054912567)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.26631414890289307)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.30605873465538025)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.18462954461574554)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.45882394909858704)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.15512453019618988)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.15366800129413605)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.1108575090765953)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.29344427585601807)\n",
            "Training set:23680/75750 (31.25 Loss:0.25501748919487)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.14865918457508087)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.3189430236816406)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.3249659538269043)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.5419424772262573)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.3618791699409485)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.26354560256004333)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.38046500086784363)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.32947635650634766)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.24849361181259155)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.24234920740127563)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.18051326274871826)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.3946341276168823)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.20855528116226196)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.14390107989311218)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.23627901077270508)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.26504969596862793)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.13996969163417816)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.329592764377594)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.21470274031162262)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.2548777759075165)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.2973589301109314)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.3756876289844513)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.1863793432712555)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.5211168527603149)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.2550441324710846)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.21202018857002258)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.3204043209552765)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.4130297303199768)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.3885795772075653)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.3645878732204437)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.469137966632843)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.30845803022384644)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.30368927121162415)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.1362406462430954)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.23738589882850647)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.22569434344768524)\n",
            "Training set:47360/75750 (62.5 Loss:0.5502562522888184)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.2989932596683502)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.21326327323913574)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.3053794205188751)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.33591288328170776)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.23632793128490448)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.1917802393436432)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.1919112652540207)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.33781328797340393)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.5452717542648315)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.15465541183948517)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.3192782402038574)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.22965694963932037)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.42311692237854004)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.40381452441215515)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.3429335057735443)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.3971758484840393)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.32992708683013916)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.36614885926246643)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.23589447140693665)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.3918703496456146)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.2856319844722748)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.2541324198246002)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.3160221576690674)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.25865480303764343)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.25592324137687683)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.2801972031593323)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.17725247144699097)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.40708649158477783)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.28489792346954346)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.2855784595012665)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.32908567786216736)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.33893951773643494)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.3508772552013397)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.4125232994556427)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.3832787573337555)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.7068718075752258)\n",
            "Training set:71040/75750 (93.75 Loss:0.45299068093299866)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.5393332839012146)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.524520754814148)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.7055543661117554)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.3368591368198395)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.44683289527893066)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.43422549962997437)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.2646748721599579)\n",
            "Training set: Average loss: 0.290388\n",
            "Validation set: Average loss: 0.883345894088692, Accuracy: 20127/25250 (79.71089108910891%)\n",
            "Validation set: Average loss: 0.883345894088692,Accuracy: 79.71%, Weighted precision:0.81, Weighted recall:0.8,Weighted f1:0.8\n",
            "Micro precision:0.8, Micro recall:0.8, Micro f1:0.8\n",
            "Epoch: 14\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.18964911997318268)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.16665352880954742)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.24931693077087402)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.2059229612350464)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.2003856748342514)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.4264117479324341)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.2629460096359253)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.17398768663406372)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.3096482455730438)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.12140193581581116)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.09337418526411057)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.16400660574436188)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.32637158036231995)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.3690439760684967)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.20313632488250732)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.3175731599330902)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.34003356099128723)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.20853595435619354)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.2379024773836136)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.2960549294948578)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.4006243944168091)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.12578660249710083)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.11554263532161713)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.08546728640794754)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.18665960431098938)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.22330383956432343)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.3920436501502991)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.2367732971906662)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.6311151385307312)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.23978553712368011)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.3009457588195801)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.2818688750267029)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.25140804052352905)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.12494802474975586)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.22340770065784454)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.3964836895465851)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.21423868834972382)\n",
            "Training set:23680/75750 (31.25 Loss:0.42634639143943787)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.2830866873264313)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.3866211771965027)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.35215243697166443)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.32285889983177185)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.29519858956336975)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.3995547890663147)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.1233760342001915)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.3905905485153198)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.20343191921710968)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.2488090544939041)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.2542087435722351)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.2947978377342224)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.45170050859451294)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.2817438840866089)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.30608880519866943)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.12755611538887024)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.3158247768878937)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.41552451252937317)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.3424563407897949)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.32960838079452515)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.3606865108013153)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.19051682949066162)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.3284148871898651)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.285298615694046)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.3367947041988373)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.4869193136692047)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.3342958390712738)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.137959286570549)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.36916229128837585)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.18631023168563843)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.4167974591255188)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.195040762424469)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.21077421307563782)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.2610761523246765)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.28059494495391846)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.30640217661857605)\n",
            "Training set:47360/75750 (62.5 Loss:0.37011006474494934)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.2651658356189728)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.07530063390731812)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.33581602573394775)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.34030601382255554)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.33470651507377625)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.5261957049369812)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.16795389354228973)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.4410560727119446)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.48883840441703796)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.3776918947696686)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.33266931772232056)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.2772841453552246)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.14795774221420288)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.4547097980976105)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.325317919254303)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.2700549066066742)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.21211500465869904)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.4240299463272095)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.2545648217201233)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.42561328411102295)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.23830178380012512)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.19834350049495697)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.5587248802185059)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.1838577836751938)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.45762404799461365)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.12286987155675888)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.2871987819671631)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.2708014249801636)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.26323533058166504)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.4874468743801117)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.27257370948791504)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.7726323008537292)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.3219854533672333)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.2531157433986664)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.22279702126979828)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.27477309107780457)\n",
            "Training set:71040/75750 (93.75 Loss:0.32123875617980957)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.1780186891555786)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.3463287353515625)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.31460073590278625)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.30645987391471863)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.3456359803676605)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.3660992681980133)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.3431539237499237)\n",
            "Training set: Average loss: 0.280240\n",
            "Validation set: Average loss: 0.8865758933032615, Accuracy: 20189/25250 (79.95643564356436%)\n",
            "Validation set: Average loss: 0.8865758933032615,Accuracy: 79.96%, Weighted precision:0.81, Weighted recall:0.8,Weighted f1:0.8\n",
            "Micro precision:0.8, Micro recall:0.8, Micro f1:0.8\n",
            "Epoch: 15\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.22878994047641754)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.18906012177467346)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.130801260471344)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.1504288911819458)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.19588765501976013)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.26730644702911377)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.20553171634674072)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.086776502430439)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.15462517738342285)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.1869373321533203)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.2064601331949234)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.12826940417289734)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.24831539392471313)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.10866188257932663)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.34592923521995544)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.25366634130477905)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.1911666840314865)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.4062836766242981)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.23109522461891174)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.24210995435714722)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.2543955445289612)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.17368362843990326)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.28576719760894775)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.20364226400852203)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.21316394209861755)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.09704521298408508)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.12881991267204285)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.16543251276016235)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.24850457906723022)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.2608841061592102)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.26968905329704285)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.2326555848121643)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.1576588749885559)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.09151831269264221)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.3433264493942261)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.42695337533950806)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.2683234214782715)\n",
            "Training set:23680/75750 (31.25 Loss:0.14876756072044373)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.23259229958057404)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.33745276927948)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.18909837305545807)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.20040461421012878)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.24872788786888123)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.2956438660621643)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.13078391551971436)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.300961434841156)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.2237011194229126)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.4681241810321808)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.2891130745410919)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.23167897760868073)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.21906782686710358)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.23425449430942535)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.24649181962013245)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.21785740554332733)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.3027752637863159)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.46201133728027344)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.2486017644405365)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.4346367418766022)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.2153996229171753)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.25136303901672363)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.3095308244228363)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.20306450128555298)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.32660964131355286)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.5259177088737488)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.41369956731796265)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.31339016556739807)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.1218406930565834)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.3055002987384796)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.20120029151439667)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.4464900493621826)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.2654176652431488)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.39226970076560974)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.25332340598106384)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.10461000353097916)\n",
            "Training set:47360/75750 (62.5 Loss:0.4163746237754822)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.2846713364124298)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.25074535608291626)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.3947594165802002)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.49759015440940857)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.41757097840309143)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.3854697048664093)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.43573421239852905)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.2895795702934265)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.09312354773283005)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.2354065626859665)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.26655757427215576)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.32400190830230713)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.3360070586204529)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.26133033633232117)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.3597007691860199)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.26025232672691345)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.39658352732658386)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.2351081669330597)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.31840068101882935)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.6814103722572327)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.33262890577316284)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.24955657124519348)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.3392525613307953)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.2922450602054596)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.5189909934997559)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.3718520998954773)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.3933809697628021)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.38906216621398926)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.39851486682891846)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.36158883571624756)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.3447142243385315)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.19525034725666046)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.10137971490621567)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.2540065348148346)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.3392086625099182)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.29448822140693665)\n",
            "Training set:71040/75750 (93.75 Loss:0.15148727595806122)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.3413662314414978)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.341810405254364)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.33360815048217773)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.3581303060054779)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.26810744404792786)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.24195313453674316)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.3240838944911957)\n",
            "Training set: Average loss: 0.261716\n",
            "Validation set: Average loss: 0.9847693891945307, Accuracy: 19650/25250 (77.82178217821782%)\n",
            "Validation set: Average loss: 0.9847693891945307,Accuracy: 77.82%, Weighted precision:0.8, Weighted recall:0.78,Weighted f1:0.78\n",
            "Micro precision:0.78, Micro recall:0.78, Micro f1:0.78\n",
            "Epoch: 16\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.23115095496177673)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.2961331307888031)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.15440335869789124)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.18754200637340546)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.5684260725975037)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.4005570709705353)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.24450837075710297)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.0669841319322586)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.12373668700456619)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.22000357508659363)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.07664844393730164)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.10274442285299301)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.21934057772159576)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.09651008993387222)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.13347454369068146)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.16909480094909668)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.3177381753921509)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.16519880294799805)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.10418786853551865)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.2888184189796448)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.15209776163101196)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.29662245512008667)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.15323470532894135)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.23353232443332672)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.2032940536737442)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.14944136142730713)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.20715908706188202)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.47183942794799805)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.15145286917686462)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.3663528561592102)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.2840019762516022)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.16503338515758514)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.35340526700019836)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.24613800644874573)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.03899873420596123)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.08897218108177185)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.2004978209733963)\n",
            "Training set:23680/75750 (31.25 Loss:0.3025975823402405)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.24569986760616302)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.19973677396774292)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.1550072282552719)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.26971468329429626)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.33569198846817017)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.22718052566051483)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.07334500551223755)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.17832987010478973)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.19079825282096863)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.15139596164226532)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.20091533660888672)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.16213972866535187)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.2686174511909485)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.25950920581817627)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.21247714757919312)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.33133867383003235)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.053830891847610474)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.12655146420001984)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.17584584653377533)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.11101974546909332)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.2543267607688904)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.24856910109519958)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.22067204117774963)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.1057083010673523)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.13554884493350983)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.07580194622278214)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.40711498260498047)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.13473916053771973)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.19051411747932434)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.3784419000148773)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.27261778712272644)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.13288381695747375)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.27031210064888)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.1729254424571991)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.22647780179977417)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.09771931171417236)\n",
            "Training set:47360/75750 (62.5 Loss:0.20475924015045166)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.42482802271842957)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.18044792115688324)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.29468464851379395)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.6488797664642334)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.15933877229690552)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.1030210554599762)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.3288730978965759)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.23649555444717407)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.1498473882675171)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.16479365527629852)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.13025328516960144)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.19387143850326538)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.27094075083732605)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.20542453229427338)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.18088175356388092)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.14176273345947266)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.10837969183921814)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.20160946249961853)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.23642534017562866)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.24187208712100983)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.08812367171049118)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.42295315861701965)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.2887236475944519)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.184791699051857)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.15461714565753937)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.27722591161727905)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.26476824283599854)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.3022845387458801)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.23571868240833282)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.43946021795272827)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.30503761768341064)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.2069510966539383)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.13374964892864227)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.35964488983154297)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.31825003027915955)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.17827144265174866)\n",
            "Training set:71040/75750 (93.75 Loss:0.20269396901130676)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.3565019369125366)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.1433926522731781)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.30931511521339417)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.20239117741584778)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.12861408293247223)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.2902107536792755)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.2212449461221695)\n",
            "Training set: Average loss: 0.242336\n",
            "Validation set: Average loss: 0.922553919581107, Accuracy: 20155/25250 (79.82178217821782%)\n",
            "Validation set: Average loss: 0.922553919581107,Accuracy: 79.82%, Weighted precision:0.81, Weighted recall:0.8,Weighted f1:0.8\n",
            "Micro precision:0.8, Micro recall:0.8, Micro f1:0.8\n",
            "Epoch: 17\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.39458364248275757)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.28567108511924744)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.17871037125587463)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.09815952926874161)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.21417847275733948)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.23884986340999603)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.2298019975423813)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.20586490631103516)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.14648078382015228)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.15342915058135986)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.1906508207321167)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.1246737688779831)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.09140969067811966)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.2440764158964157)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.16426295042037964)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.153289794921875)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.20234160125255585)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.07271134853363037)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.3142954707145691)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.3127298653125763)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.4296198785305023)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.18486623466014862)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.11053110659122467)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.1689184010028839)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.2675100266933441)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.4247303605079651)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.31119710206985474)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.1536693125963211)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.1565871685743332)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.1830616295337677)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.1712483912706375)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.22862930595874786)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.27738040685653687)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.23912544548511505)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.18640045821666718)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.2754877209663391)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.11796252429485321)\n",
            "Training set:23680/75750 (31.25 Loss:0.2372753769159317)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.08305304497480392)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.12316769361495972)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.17761273682117462)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.18356680870056152)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.26017072796821594)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.17407165467739105)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.17266780138015747)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.09454531967639923)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.2402135580778122)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.1736566573381424)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.2576754093170166)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.10964340716600418)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.21431945264339447)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.2588723599910736)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.18810002505779266)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.10470940917730331)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.2364163100719452)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.11534503102302551)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.13516469299793243)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.22769153118133545)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.0902121365070343)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.14156632125377655)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.3588216304779053)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.2166833132505417)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.4005979299545288)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.14084933698177338)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.2296074628829956)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.10011954605579376)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.35055574774742126)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.21717146039009094)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.25374355912208557)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.18272937834262848)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.15189918875694275)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.3855176568031311)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.3900623321533203)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.30599477887153625)\n",
            "Training set:47360/75750 (62.5 Loss:0.3660357594490051)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.19272328913211823)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.2541055977344513)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.4164854884147644)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.251321405172348)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.22018137574195862)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.21402835845947266)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.24074314534664154)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.1491278111934662)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.30418506264686584)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.24084369838237762)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.1592302769422531)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.08342505991458893)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.22661232948303223)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.22743819653987885)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.4560892581939697)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.1304682195186615)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.12804564833641052)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.22365184128284454)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.2910037040710449)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.24139852821826935)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.25297456979751587)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.27773866057395935)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.22459669411182404)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.10574237257242203)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.340587317943573)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.3923567831516266)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.2932978570461273)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.38159891963005066)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.1413366198539734)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.3097270727157593)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.1688031256198883)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.3192003071308136)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.26189643144607544)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.2293372005224228)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.3057152032852173)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.10559101402759552)\n",
            "Training set:71040/75750 (93.75 Loss:0.21672891080379486)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.30375194549560547)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.5220012664794922)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.3842557370662689)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.21093690395355225)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.34679555892944336)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.20996080338954926)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.3564590513706207)\n",
            "Training set: Average loss: 0.227665\n",
            "Validation set: Average loss: 0.992294679467914, Accuracy: 19870/25250 (78.6930693069307%)\n",
            "Validation set: Average loss: 0.992294679467914,Accuracy: 78.69%, Weighted precision:0.8, Weighted recall:0.79,Weighted f1:0.79\n",
            "Micro precision:0.79, Micro recall:0.79, Micro f1:0.79\n",
            "Epoch: 18\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.11148148775100708)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.19172221422195435)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.21725568175315857)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.06416492164134979)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.2501431405544281)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.21009856462478638)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.17680087685585022)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.18222452700138092)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.036923594772815704)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.11313381046056747)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.11969316750764847)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.12394315749406815)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.33640217781066895)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.11692991107702255)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.05811390280723572)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.21393191814422607)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.21966664493083954)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.26256805658340454)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.1729472428560257)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.0871891900897026)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.264891117811203)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.12778322398662567)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.148814395070076)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.11159012466669083)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.35823267698287964)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.14522463083267212)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.10381710529327393)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.2592330873012543)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.26116737723350525)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.11947465687990189)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.08916933089494705)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.24774675071239471)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.21760697662830353)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.10498331487178802)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.16145777702331543)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.2356368750333786)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.24594013392925262)\n",
            "Training set:23680/75750 (31.25 Loss:0.2464882880449295)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.12397277355194092)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.2197842299938202)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.17073579132556915)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.18918855488300323)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.2350112348794937)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.25903239846229553)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.31120607256889343)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.16681404411792755)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.24200667440891266)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.33093157410621643)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.2624770700931549)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.30173438787460327)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.5085547566413879)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.21567320823669434)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.2582961618900299)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.20615118741989136)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.0921759381890297)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.22209128737449646)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.21608364582061768)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.06284131109714508)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.14388597011566162)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.3967837393283844)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.11479204893112183)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.18590478599071503)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.3491114377975464)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.23502489924430847)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.43489357829093933)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.22052894532680511)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.18056851625442505)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.10254231840372086)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.33864066004753113)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.21281278133392334)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.2349257916212082)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.2152452915906906)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.3059689700603485)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.30323296785354614)\n",
            "Training set:47360/75750 (62.5 Loss:0.18293127417564392)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.37910810112953186)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.19927763938903809)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.33788734674453735)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.1755133420228958)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.2255803495645523)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.18275846540927887)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.13006164133548737)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.1756982058286667)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.41976985335350037)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.3407900929450989)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.3268757164478302)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.2475116103887558)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.27424731850624084)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.18122757971286774)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.06377020478248596)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.20130334794521332)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.40299391746520996)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.2701421082019806)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.21656590700149536)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.27285462617874146)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.272654265165329)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.2932854890823364)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.39876723289489746)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.1312435269355774)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.2925593852996826)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.2994724214076996)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.2598404586315155)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.23432698845863342)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.3809341490268707)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.17208515107631683)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.2502901256084442)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.23089560866355896)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.05720776319503784)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.28846052289009094)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.2198357731103897)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.2982535660266876)\n",
            "Training set:71040/75750 (93.75 Loss:0.1508409082889557)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.40256667137145996)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.2725695073604584)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.22763125598430634)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.27396589517593384)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.3458886742591858)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.36571961641311646)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.2725573778152466)\n",
            "Training set: Average loss: 0.219677\n",
            "Validation set: Average loss: 0.9513339610628878, Accuracy: 20152/25250 (79.80990099009901%)\n",
            "Validation set: Average loss: 0.9513339610628878,Accuracy: 79.81%, Weighted precision:0.81, Weighted recall:0.8,Weighted f1:0.8\n",
            "Micro precision:0.8, Micro recall:0.8, Micro f1:0.8\n",
            "Epoch: 19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.3452097177505493)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.1593666523694992)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.12181909382343292)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.03636578097939491)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.07516323029994965)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.10623341798782349)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.13180094957351685)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.10111507028341293)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.07931889593601227)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.09032327681779861)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.09924790263175964)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.21207605302333832)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.10725440084934235)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.1730244904756546)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.22802339494228363)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.17494100332260132)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.1454867124557495)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.13628578186035156)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.19192056357860565)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.12240036576986313)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.18525908887386322)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.27326688170433044)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.07601210474967957)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.10754228383302689)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.10242106020450592)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.14220577478408813)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.14435578882694244)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.19893905520439148)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.14181095361709595)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.23327293992042542)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.2516229748725891)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.14151054620742798)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.1951003521680832)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.10453640669584274)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.19017961621284485)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.18289895355701447)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.09181062132120132)\n",
            "Training set:23680/75750 (31.25 Loss:0.2689501941204071)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.1435769945383072)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.11579613387584686)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.19741089642047882)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.20593777298927307)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.26068899035453796)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.07914102077484131)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.32978901267051697)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.13120360672473907)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.024317800998687744)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.20996807515621185)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.2168601006269455)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.21215029060840607)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.15379701554775238)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.18404898047447205)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.2326880395412445)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.19881248474121094)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.12860800325870514)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.3722376525402069)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.1737525910139084)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.11387232691049576)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.15419737994670868)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.23720097541809082)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.10458860546350479)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.27109673619270325)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.23559600114822388)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.28788313269615173)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.08196669071912766)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.3190537393093109)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.1340361386537552)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.38460591435432434)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.12217710167169571)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.4047483503818512)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.20631909370422363)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.0727287009358406)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.40932729840278625)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.3273223042488098)\n",
            "Training set:47360/75750 (62.5 Loss:0.17060114443302155)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.15377311408519745)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.10533631592988968)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.3165352940559387)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.26107871532440186)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.2383059710264206)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.18549281358718872)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.2862868905067444)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.13822895288467407)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.27680012583732605)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.38438665866851807)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.19635960459709167)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.2412748783826828)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.2360774576663971)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.12003312259912491)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.32288241386413574)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.2533433139324188)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.27450230717658997)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.3713882565498352)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.24774958193302155)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.4325605630874634)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.23770767450332642)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.1614406257867813)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.35857152938842773)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.3246355652809143)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.5332356691360474)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.3250766396522522)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.18941625952720642)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.18287096917629242)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.10824397206306458)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.31392383575439453)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.4382440447807312)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.220357283949852)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.21002022922039032)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.24703681468963623)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.05314771831035614)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.1087096557021141)\n",
            "Training set:71040/75750 (93.75 Loss:0.47673720121383667)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.1613452136516571)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.3258249759674072)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.22357158362865448)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.36464816331863403)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.28971561789512634)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.06602582335472107)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.17513714730739594)\n",
            "Training set: Average loss: 0.200620\n",
            "Validation set: Average loss: 1.063649737640036, Accuracy: 19691/25250 (77.98415841584159%)\n",
            "Validation set: Average loss: 1.063649737640036,Accuracy: 77.98%, Weighted precision:0.8, Weighted recall:0.78,Weighted f1:0.78\n",
            "Micro precision:0.78, Micro recall:0.78, Micro f1:0.78\n",
            "Epoch: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set:0/75750 (0.0 Loss:0.16552715003490448)\n",
            "Training set:640/75750 (0.8445945945945946 Loss:0.1452663093805313)\n",
            "Training set:1280/75750 (1.6891891891891893 Loss:0.21222837269306183)\n",
            "Training set:1920/75750 (2.5337837837837838 Loss:0.04939998686313629)\n",
            "Training set:2560/75750 (3.3783783783783785 Loss:0.17864705622196198)\n",
            "Training set:3200/75750 (4.222972972972973 Loss:0.07855719327926636)\n",
            "Training set:3840/75750 (5.0675675675675675 Loss:0.10485973209142685)\n",
            "Training set:4480/75750 (5.912162162162162 Loss:0.23823800683021545)\n",
            "Training set:5120/75750 (6.756756756756757 Loss:0.07270675897598267)\n",
            "Training set:5760/75750 (7.601351351351352 Loss:0.27663880586624146)\n",
            "Training set:6400/75750 (8.445945945945946 Loss:0.19189287722110748)\n",
            "Training set:7040/75750 (9.29054054054054 Loss:0.31784072518348694)\n",
            "Training set:7680/75750 (10.135135135135135 Loss:0.39238426089286804)\n",
            "Training set:8320/75750 (10.97972972972973 Loss:0.4047732651233673)\n",
            "Training set:8960/75750 (11.824324324324325 Loss:0.055590853095054626)\n",
            "Training set:9600/75750 (12.66891891891892 Loss:0.052930861711502075)\n",
            "Training set:10240/75750 (13.513513513513514 Loss:0.21402615308761597)\n",
            "Training set:10880/75750 (14.358108108108109 Loss:0.06362465769052505)\n",
            "Training set:11520/75750 (15.202702702702704 Loss:0.0730496421456337)\n",
            "Training set:12160/75750 (16.0472972972973 Loss:0.10287857800722122)\n",
            "Training set:12800/75750 (16.89189189189189 Loss:0.2167566865682602)\n",
            "Training set:13440/75750 (17.736486486486488 Loss:0.3365304470062256)\n",
            "Training set:14080/75750 (18.58108108108108 Loss:0.15431039035320282)\n",
            "Training set:14720/75750 (19.425675675675677 Loss:0.2074374407529831)\n",
            "Training set:15360/75750 (20.27027027027027 Loss:0.1277071237564087)\n",
            "Training set:16000/75750 (21.114864864864863 Loss:0.2623479664325714)\n",
            "Training set:16640/75750 (21.95945945945946 Loss:0.287300169467926)\n",
            "Training set:17280/75750 (22.804054054054053 Loss:0.0642179399728775)\n",
            "Training set:17920/75750 (23.64864864864865 Loss:0.2771250307559967)\n",
            "Training set:18560/75750 (24.493243243243242 Loss:0.12396343052387238)\n",
            "Training set:19200/75750 (25.33783783783784 Loss:0.3028688132762909)\n",
            "Training set:19840/75750 (26.18243243243243 Loss:0.27235424518585205)\n",
            "Training set:20480/75750 (27.027027027027028 Loss:0.0610700361430645)\n",
            "Training set:21120/75750 (27.87162162162162 Loss:0.16983388364315033)\n",
            "Training set:21760/75750 (28.716216216216218 Loss:0.2056754231452942)\n",
            "Training set:22400/75750 (29.56081081081081 Loss:0.20928707718849182)\n",
            "Training set:23040/75750 (30.405405405405407 Loss:0.1906980276107788)\n",
            "Training set:23680/75750 (31.25 Loss:0.1705293357372284)\n",
            "Training set:24320/75750 (32.0945945945946 Loss:0.21724966168403625)\n",
            "Training set:24960/75750 (32.939189189189186 Loss:0.09054730087518692)\n",
            "Training set:25600/75750 (33.78378378378378 Loss:0.06956439465284348)\n",
            "Training set:26240/75750 (34.62837837837838 Loss:0.15067386627197266)\n",
            "Training set:26880/75750 (35.472972972972975 Loss:0.10818719863891602)\n",
            "Training set:27520/75750 (36.317567567567565 Loss:0.3671400547027588)\n",
            "Training set:28160/75750 (37.16216216216216 Loss:0.3126874268054962)\n",
            "Training set:28800/75750 (38.00675675675676 Loss:0.31336092948913574)\n",
            "Training set:29440/75750 (38.851351351351354 Loss:0.1348208636045456)\n",
            "Training set:30080/75750 (39.695945945945944 Loss:0.16984422504901886)\n",
            "Training set:30720/75750 (40.54054054054054 Loss:0.1715477705001831)\n",
            "Training set:31360/75750 (41.38513513513514 Loss:0.3171137571334839)\n",
            "Training set:32000/75750 (42.229729729729726 Loss:0.13742971420288086)\n",
            "Training set:32640/75750 (43.07432432432432 Loss:0.24940186738967896)\n",
            "Training set:33280/75750 (43.91891891891892 Loss:0.29968273639678955)\n",
            "Training set:33920/75750 (44.763513513513516 Loss:0.12063385546207428)\n",
            "Training set:34560/75750 (45.608108108108105 Loss:0.147939071059227)\n",
            "Training set:35200/75750 (46.4527027027027 Loss:0.04619913548231125)\n",
            "Training set:35840/75750 (47.2972972972973 Loss:0.20715539157390594)\n",
            "Training set:36480/75750 (48.141891891891895 Loss:0.15147773921489716)\n",
            "Training set:37120/75750 (48.986486486486484 Loss:0.18105824291706085)\n",
            "Training set:37760/75750 (49.83108108108108 Loss:0.07620908319950104)\n",
            "Training set:38400/75750 (50.67567567567568 Loss:0.2315998673439026)\n",
            "Training set:39040/75750 (51.520270270270274 Loss:0.21528048813343048)\n",
            "Training set:39680/75750 (52.36486486486486 Loss:0.1081073209643364)\n",
            "Training set:40320/75750 (53.20945945945946 Loss:0.3204517662525177)\n",
            "Training set:40960/75750 (54.054054054054056 Loss:0.24499733746051788)\n",
            "Training set:41600/75750 (54.898648648648646 Loss:0.26231396198272705)\n",
            "Training set:42240/75750 (55.74324324324324 Loss:0.14899712800979614)\n",
            "Training set:42880/75750 (56.58783783783784 Loss:0.24405069649219513)\n",
            "Training set:43520/75750 (57.432432432432435 Loss:0.16048911213874817)\n",
            "Training set:44160/75750 (58.277027027027025 Loss:0.18854494392871857)\n",
            "Training set:44800/75750 (59.12162162162162 Loss:0.28614190220832825)\n",
            "Training set:45440/75750 (59.96621621621622 Loss:0.1296795755624771)\n",
            "Training set:46080/75750 (60.810810810810814 Loss:0.21133026480674744)\n",
            "Training set:46720/75750 (61.6554054054054 Loss:0.1368260383605957)\n",
            "Training set:47360/75750 (62.5 Loss:0.26683998107910156)\n",
            "Training set:48000/75750 (63.3445945945946 Loss:0.0835827961564064)\n",
            "Training set:48640/75750 (64.1891891891892 Loss:0.21978946030139923)\n",
            "Training set:49280/75750 (65.03378378378379 Loss:0.21534454822540283)\n",
            "Training set:49920/75750 (65.87837837837837 Loss:0.10555243492126465)\n",
            "Training set:50560/75750 (66.72297297297297 Loss:0.13777847588062286)\n",
            "Training set:51200/75750 (67.56756756756756 Loss:0.26451754570007324)\n",
            "Training set:51840/75750 (68.41216216216216 Loss:0.2578568756580353)\n",
            "Training set:52480/75750 (69.25675675675676 Loss:0.2551173269748688)\n",
            "Training set:53120/75750 (70.10135135135135 Loss:0.3405539095401764)\n",
            "Training set:53760/75750 (70.94594594594595 Loss:0.23363742232322693)\n",
            "Training set:54400/75750 (71.79054054054055 Loss:0.29730236530303955)\n",
            "Training set:55040/75750 (72.63513513513513 Loss:0.24100065231323242)\n",
            "Training set:55680/75750 (73.47972972972973 Loss:0.4731265902519226)\n",
            "Training set:56320/75750 (74.32432432432432 Loss:0.35516199469566345)\n",
            "Training set:56960/75750 (75.16891891891892 Loss:0.12837214767932892)\n",
            "Training set:57600/75750 (76.01351351351352 Loss:0.29648905992507935)\n",
            "Training set:58240/75750 (76.85810810810811 Loss:0.13595892488956451)\n",
            "Training set:58880/75750 (77.70270270270271 Loss:0.24894919991493225)\n",
            "Training set:59520/75750 (78.54729729729729 Loss:0.31217843294143677)\n",
            "Training set:60160/75750 (79.39189189189189 Loss:0.22239333391189575)\n",
            "Training set:60800/75750 (80.23648648648648 Loss:0.12619538605213165)\n",
            "Training set:61440/75750 (81.08108108108108 Loss:0.234618678689003)\n",
            "Training set:62080/75750 (81.92567567567568 Loss:0.18255110085010529)\n",
            "Training set:62720/75750 (82.77027027027027 Loss:0.29193681478500366)\n",
            "Training set:63360/75750 (83.61486486486487 Loss:0.14653484523296356)\n",
            "Training set:64000/75750 (84.45945945945945 Loss:0.16355018317699432)\n",
            "Training set:64640/75750 (85.30405405405405 Loss:0.16507670283317566)\n",
            "Training set:65280/75750 (86.14864864864865 Loss:0.14709633588790894)\n",
            "Training set:65920/75750 (86.99324324324324 Loss:0.2403726875782013)\n",
            "Training set:66560/75750 (87.83783783783784 Loss:0.21938611567020416)\n",
            "Training set:67200/75750 (88.68243243243244 Loss:0.2074078917503357)\n",
            "Training set:67840/75750 (89.52702702702703 Loss:0.16545143723487854)\n",
            "Training set:68480/75750 (90.37162162162163 Loss:0.3283856213092804)\n",
            "Training set:69120/75750 (91.21621621621621 Loss:0.3212348222732544)\n",
            "Training set:69760/75750 (92.0608108108108 Loss:0.21967823803424835)\n",
            "Training set:70400/75750 (92.9054054054054 Loss:0.2248455435037613)\n",
            "Training set:71040/75750 (93.75 Loss:0.07099304348230362)\n",
            "Training set:71680/75750 (94.5945945945946 Loss:0.30220481753349304)\n",
            "Training set:72320/75750 (95.4391891891892 Loss:0.29375410079956055)\n",
            "Training set:72960/75750 (96.28378378378379 Loss:0.30324703454971313)\n",
            "Training set:73600/75750 (97.12837837837837 Loss:0.33360975980758667)\n",
            "Training set:74240/75750 (97.97297297297297 Loss:0.3510238826274872)\n",
            "Training set:74880/75750 (98.81756756756756 Loss:0.5055832266807556)\n",
            "Training set:75520/75750 (99.66216216216216 Loss:0.32557976245880127)\n",
            "Training set: Average loss: 0.201019\n",
            "Validation set: Average loss: 1.0342068020882131, Accuracy: 19800/25250 (78.41584158415841%)\n",
            "Validation set: Average loss: 1.0342068020882131,Accuracy: 78.42%, Weighted precision:0.8, Weighted recall:0.78,Weighted f1:0.79\n",
            "Micro precision:0.78, Micro recall:0.78, Micro f1:0.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(epoch_nums, training_loss)\n",
        "plt.plot(epoch_nums, validation_loss)\n",
        "plt.plot(epoch_nums, accuracy)\n",
        "plt.title('model Performance')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend(['training', 'validation','accuracy'], loc='center right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "Ym4qSVXjmx3O",
        "outputId": "e8ca0c31-07a0-4e4b-9511-fa1f2eba09fb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}